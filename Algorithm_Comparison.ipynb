{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBE0vRE4DTtR"
      },
      "source": [
        "# **CS 5361/6361 Machine Learning**\n",
        "\n",
        "**Classifying Fashion MNIST and predicting particle behavior using regression algorithms, while evaluating performance based on running time and accuracy for classification and mean squared error for regression.**\n",
        "\n",
        "**Authors:** <br>\n",
        "Ruben Martinez <br>\n",
        "Francis Owusu Dampare <br>\n",
        "**Last modified:** 10/17/2024<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HVDJeGx01rv"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files, drive\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import all 8 classifiers and regressors.\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTPiAoXj1vvX"
      },
      "source": [
        "# **1. Determining which classification algorithm works best for the fashion MNIST dataset, in terms of running time and accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SFaFXH31e7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67576707-052a-44ef-98e1-a9a55cfc9222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1 and flatten\n",
        "X_train = np.float32(X_train.reshape(X_train.shape[0],-1)/255)\n",
        "X_test = np.float32(X_test.reshape(X_test.shape[0],-1)/255)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOaySBe62DEc",
        "outputId": "e4b9bdba-f7d6-4895-e52f-2914ed6cb168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier          Training Time (s)   Testing Time (s)    Training Acc   Testing Acc    \n",
            "------------------------------------------------------------------------------------------\n",
            "K Nearest Neighbors 0.0369              43.6773             0.8998         0.8554         \n",
            "Confusion matrix:\n",
            "[[855   1  17  16   3   1 100   1   6   0]\n",
            " [  8 968   4  12   4   0   3   0   1   0]\n",
            " [ 24   2 819  11  75   0  69   0   0   0]\n",
            " [ 41   8  15 860  39   0  34   0   3   0]\n",
            " [  2   1 126  26 773   0  71   0   1   0]\n",
            " [  1   0   0   0   0 822   5  96   1  75]\n",
            " [176   1 132  23  80   0 575   0  13   0]\n",
            " [  0   0   0   0   0   3   0 961   0  36]\n",
            " [  2   0  10   4   7   0  16   7 953   1]\n",
            " [  0   0   0   0   0   2   1  29   0 968]]\n",
            "\n",
            "Decision Tree       47.6041             0.0086              1.0000         0.8010         \n",
            "Confusion matrix:\n",
            "[[736   4  25  46  10   3 163   0  12   1]\n",
            " [  8 948   4  25   5   0   8   0   2   0]\n",
            " [ 24   2 688  16 144   0 118   0   7   1]\n",
            " [ 44  33  16 777  58   0  64   0   6   2]\n",
            " [  8   2 149  39 680   0 116   0   6   0]\n",
            " [  2   0   1   3   0 898   2  60   7  27]\n",
            " [150   7 116  40 101   1 557   0  28   0]\n",
            " [  0   0   0   0   0  53   0 894   4  49]\n",
            " [  8   3  10   6   7  12  16   8 928   2]\n",
            " [  1   0   0   0   1  25   0  66   3 904]]\n",
            "\n",
            "Random Forest       110.9465            0.3457              1.0000         0.8769         \n",
            "Confusion matrix:\n",
            "[[871   1  12  29   3   1  73   0  10   0]\n",
            " [  4 961   3  22   4   0   4   0   2   0]\n",
            " [ 13   0 792  11 123   0  56   0   5   0]\n",
            " [ 21   2  13 905  33   0  24   0   2   0]\n",
            " [  1   1  95  32 813   0  55   0   3   0]\n",
            " [  0   0   0   1   0 956   0  29   1  13]\n",
            " [154   1 121  29  83   0 594   0  18   0]\n",
            " [  0   0   0   0   0  10   0 955   0  35]\n",
            " [  1   2   4   2   3   1   8   5 974   0]\n",
            " [  0   0   0   0   0   9   0  40   3 948]]\n",
            "\n",
            "Logistic Regression 32.6241             0.0629              0.8660         0.8438         \n",
            "Confusion matrix:\n",
            "[[809   5  18  49   5   2 101   0  11   0]\n",
            " [  4 961   1  25   5   0   3   0   1   0]\n",
            " [ 20   5 739  14 128   1  84   0   9   0]\n",
            " [ 31  13  13 868  30   0  41   0   4   0]\n",
            " [  0   2 113  38 756   1  82   0   8   0]\n",
            " [  0   0   0   1   0 929   0  49   3  18]\n",
            " [130   4 135  39 104   1 565   0  22   0]\n",
            " [  0   0   0   0   0  35   0 935   0  30]\n",
            " [  9   1   6  10   3   5  20   5 941   0]\n",
            " [  0   0   0   0   0  18   0  45   2 935]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifiers = {'K Nearest Neighbors' : KNeighborsClassifier(metric='euclidean',n_jobs=-1),\n",
        "               'Decision Tree' : DecisionTreeClassifier(criterion='entropy',splitter='best'),\n",
        "               'Random Forest' : RandomForestClassifier(criterion='entropy',n_jobs=-1),\n",
        "               'Logistic Regression' : LogisticRegression(n_jobs=-1)\n",
        "               }\n",
        "\n",
        "# Table header\n",
        "print(f'{\"Classifier\":<20}{\"Training Time (s)\":<20}{\"Testing Time (s)\":<20}{\"Training Acc\":<15}{\"Testing Acc\":<15}\\n{\"-\" * 90}')\n",
        "\n",
        "for classifier_name, model in classifiers.items():\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end = time.time()\n",
        "    training_time = end - start\n",
        "\n",
        "    train_pred = model.predict(X_train)\n",
        "    training_accuracy = accuracy_score(y_train, train_pred)\n",
        "\n",
        "    start = time.time()\n",
        "    test_pred = model.predict(X_test)\n",
        "    end = time.time()\n",
        "    testing_time = end - start\n",
        "\n",
        "    testing_accuracy = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    print(f'{classifier_name:<20}{training_time:<20.4f}{testing_time:<20.4f}{training_accuracy:<15.4f}{testing_accuracy:<15.4f}')\n",
        "    print(f'Confusion matrix:\\n{confusion_matrix(y_test, test_pred)}\\n')\n",
        "    # print(f'Precision: {precision_score(y_test, test_pred):6.4f}')\n",
        "    # print(f'Recall: {recall_score(y_test, test_pred):6.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Determining which regression algorithm works best for the particles dataset, in terms of running time and mean squared error.**"
      ],
      "metadata": {
        "id": "8HuLLWgcDPCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "X = np.load('/content/drive/My Drive/Colab Notebooks/particles dataset/particles_X.npy')\n",
        "y = np.load('/content/drive/My Drive/Colab Notebooks/particles dataset/particles_y.npy')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "1oJiAm_yE1SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4abf004-37f0-4a7b-cf71-47187733d998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(1419954, 5)\n",
            "(1419954,)\n",
            "(354989, 5)\n",
            "(354989,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "regressors = {\n",
        "    'K Nearest Neighbors': KNeighborsRegressor(metric='euclidean',n_jobs=-1),\n",
        "    'Decision Tree': DecisionTreeRegressor(criterion='squared_error',splitter='best'),\n",
        "    'Linear Regression': LinearRegression(n_jobs=-1),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=5, criterion='squared_error',n_jobs=-1)\n",
        "}\n",
        "\n",
        "# Table header\n",
        "print(f'{\"Regressor\":<20}{\"Training Time (s)\":<20}{\"Testing Time (s)\":<20}{\"MSE\":<10}{\"MAE\":<10}\\n{\"-\" * 90}')\n",
        "\n",
        "for regressor_name, model in regressors.items():\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end = time.time()\n",
        "    training_time = end - start\n",
        "\n",
        "    start = time.time()\n",
        "    pred = model.predict(X_test)\n",
        "    end = time.time()\n",
        "    testing_time = end - start\n",
        "\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "\n",
        "    print(f'{regressor_name:<20}{training_time:<20.4f}{testing_time:<20.4f}{mse:<10.4f}{mae:<10.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3yxhze1NtqA",
        "outputId": "3a11de5d-be6c-4b33-ba35-05fabdd42c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regressor           Training Time (s)   Testing Time (s)    MSE       MAE       \n",
            "------------------------------------------------------------------------------------------\n",
            "K Nearest Neighbors 3.8346              15.3108             0.0441    0.1705    \n",
            "Decision Tree       23.1917             0.7474              0.0759    0.2195    \n",
            "Linear Regression   0.1757              0.0038              0.0435    0.1683    \n",
            "Random Forest       63.8585             2.7343              0.0458    0.1732    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Discussing results in terms of algorithms and dataset characteristics.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2LBEvg-XPHHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "The classification algorithms we have covered in the course are the **K-Nearest Neighbors (KNN) algorithm**, **Decision Tree algorithm**, **Random Forest algorithm**, and **Logistic Regression**.\n",
        "The dataset used was the fashion MNIST data. This is a dataset which has 70000 instances with 784 features. 60000 of these instances were used for the training of the algorithms and 10000 instances were used for the testing.\n",
        "\n",
        "### Running Times\n",
        "\n",
        "The order from fastest to slowest in terms of training times are: KNN, Logistic Regression, Decision tree, Random Forest.\n",
        "\n",
        "The order from fastest to slowest in terms of testing times are: Decision Tree, Logistic Regression, Random Forest, KNN.\n",
        "\n",
        "The K-Nearest-Neighbors algorithm had the fastest training time of approximately **0.04s** and the Random Forest algorithm had the slowest training time of approximately **110.95s**. This makes sense, because KNN is a lazy learning algorithm, which doesn't actually modify the data or create any data structure, while Random Forests isn't a lazy learning algorithm and took longer because it spent time creating the trees.  Following Random Forests, the second slowest algorithm was Decision tree. This is intuitive since a single Decision tree should be faster than multiple trees in the Random Forest. Following the Decision Tree as the third slowest was Logistic regression with a training time of **32.62s**. Regarding the testing time, however, the Decision Tree algorithm had the least time of **0.01s** with  KNN algorithm having the highest of **43.68s**. Since the KNN algorithm had the fastest training time but slowest testing time this leads us to believe that there is a tradeoff between testing and training times for algorithms. That is, if the training time is fast, then the testing will be slow and vice versa. Logistic Regression and Random Forest algorithm had the second  and third least testing times of **0.06s** and **0.35s** respectively. This shows the pros and cons of the classification algorithms. K-nn algorithm is the fastest when it comes to training and slowest with testing. Decision Tree is relatively slow in training but fast in testing.\n",
        "\n",
        "### Accuracy\n",
        "On accuracy, all four classification algorithms seems to have similar values for both training and testing. These values are similarly reflected in the Confusion Matrix generated for all four algorithms; the predicated values are similar. Interestingly, from the Confusion Matrix, it can be observed that many instances of Class 6 get predicated as Class 0 by all the classifiers. If we look at the fashion mnist dataset, it makes sense that Class 6 (shirts) get misclassified as Class 0 (t-shirt/top) by all the classifiers. When looking at the fashion MNIST dataset, these two categories have very similar visual features, which makes it harder for the algorithms to differentiate between them. Shirts and t-shirts/tops can both have short sleeves and similar shapes, causing confusion in the models.\n",
        "\n",
        "Regarding algorithm performance, KNN performed the the second best in terms of accuracy; with an accuracy of **.8554**. This is likely because KNN directly compares the test instance to the training instances, allowing it to handle subtle differences between classes more effectively. The drawback is that it took the longest to test since KNN evaluates each test instance against the entire training set.\n",
        "\n",
        "On the other hand, Random Forests performed the best in terms of accuracy, with an accuracy of **.8769**. This is likely because Random Forests reduce the problem of overfitting by averaging the results of multiple decision trees, leading to better generalization on unseen data. Unlike Decision Trees, which are prone to overfitting due to their tendency to memorize the training data, Random Forests introduce randomness by building each tree on different subsets of the data. This helps them perform well, even on high-dimensional datasets like Fashion MNIST.\n",
        "\n",
        "In summary, **KNN** can be considered the best overall algorithm for the Fashion MNIST dataset when considering both running time and accuracy. While Random Forests achieved the highest accuracy, they took significantly longer to train and test (110.95 seconds for training). KNN, on the other hand, offers a solid balance—achieving the second-best accuracy while taking considerably less time to train and test. Its strength lies in not overfitting the data, but the tradeoff is the longer testing time. Given the criteria of both running time and accuracy, KNN emerges as the most practical choice, providing competitive performance without the excessive time overhead of Random Forests.\n",
        "\n",
        "## Regression\n",
        "The regression algorithms we covered in this course are the Nearest Neighbors (KNN), Decision Tree, Random Forest, and Linear Regression. The dataset used was the Particles dataset, consisting of 1,774,943 records with 5 features. We used 80% (1,419,954 records) for training and 20% (345,989 records) for testing.\n",
        "\n",
        "### Running Time\n",
        "In terms of running time, Linear Regression had the fastest performance, with a training time of **0.18** seconds and the shortest testing time of **0.004** seconds. Random Forest had the longest training time at **63.86** seconds and the second-longest testing time at **2.73** seconds. KNN had the fourth-longest training time of **3.83** seconds, but it had the slowest testing time of 15.31 seconds. The Random Forest algorithm’s performance was constrained due to using only 5 trees, as the default number of trees caused the system to crash.\n",
        "\n",
        "### Performance\n",
        "In terms of Mean Squared Error (MSE), Linear Regression again performed the best, achieving an MSE of 0.0435, making it the most accurate regressor. KNN had the second-best MSE of 0.0441, followed by Random Forest with 0.0458. The Decision Tree algorithm had the worst MSE at 0.0759.\n",
        "\n",
        "Considering both running time and MSE, **Linear Regression** emerges as the best regressor for the Particles dataset. The dataset consists of solar proton flux measurements taken at five different times in the past, with the goal of predicting the flux two hours into the future. This type of dataset, with continuous values over time, is well-suited for Linear Regression due to its simplicity and ability to model linear relationships effectively. Since solar proton flux likely follows a relatively smooth, continuous trend over time, Linear Regression can capture the general pattern without overfitting the data, which explains why it performed the best in terms of both running time and accuracy (MSE). Its ability to handle this type of time-series data efficiently makes it an ideal choice for this task. It achieved the lowest error rate and was also the fastest to train and test. KNN performed well in terms of MSE but was significantly slower in testing, which makes Linear Regression the most practical and efficient choice for this dataset."
      ],
      "metadata": {
        "id": "gqiRXSEWEa1L"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}